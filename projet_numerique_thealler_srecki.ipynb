{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet numérique : Calcul Différentiel - Lignes de niveau\n",
    "\n",
    "*Alexandre Thealler et Camille Srecki*\n",
    "\n",
    "On cherche à développer un programme python permettant de calculer les __lignes de niveau__ d'une fontion $f$ de deux variables réelles à valeurs réelles supposée continûment différentiable, c'est-à-dire les ensembles de la forme :\n",
    "\n",
    "$\\{(x,y) \\in \\mathbb{R}^2$ $\\vert$  $f(x,y) = c \\}$ où $c \\in \\mathbb{R}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import*\n",
    "import matplotlib.pyplot as plt\n",
    "import autograd \n",
    "from autograd import numpy as np\n",
    "from IPython.display import Image \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contour Simple\n",
    "\n",
    "On suppose dans un premier temps que la fontion $f$ est définie dans le __carré unité__ $[0,1]^2$ et on limite notre recherche aux lignes de niveaux qui possèdent un __point sur l'arête gauche du domaine de définition__ (de la forme $(0,y)$ pour $0 \\le y \\le 1$.) \n",
    "\n",
    "#### Amorce \n",
    "\n",
    "* On a $f$ une fonction continûment différentiable :\n",
    "\n",
    "$$\\begin{array}{ccccc}\n",
    "f & : & [0,1]^2 & \\to & \\mathbb{R} \\\\\n",
    "& & (x,y) & \\mapsto & f(x,y) \\\\\n",
    "\\end{array}$$\n",
    "\n",
    "* On définit $h$ la deuxième application partielle de $f$ au point $(0,0)$. \n",
    "\n",
    "$$\\begin{array}{ccccc}\n",
    "h & : & [0,1] & \\to & \\mathbb{R} \\\\\n",
    "& & t & \\mapsto & f(0,t) \\\\\n",
    "\\end{array}$$\n",
    "\n",
    "$h$ est __continue__ car $f$ est continûment différentiable.\n",
    "\n",
    "On cherche une condition raisonnable portant sur $f(0,0)$, $f(0,1)$ et $c \\in \\mathbb{R}$ telle qu'on soit certain qu'il existe un $t \\in [0,1]$  tel que $f(0,t) = c$, soit telle que $h(t) = c$.\n",
    "\n",
    "* Si $c \\in [\\min_{[0,1]}(h(t)),\\max_{[0,1]}(h(t))]$, alors d'après le Théorème des Valeurs Intermédiaires, l'équation $h(t) =c$ pour $t \\in [0,1]$ admet au minimum une solution.\n",
    "\n",
    "* En particulier, si $c \\in [\\min(h(0),h(1)), \\max(h(0),h(1))] \\subset [\\min_{[0,1]}(h(t)),\\max_{[0,1]}(h(t))]$, alors d'après le TVI, l'équation $h(t) =c$ pour $t \\in [0,1]$ admet au minimum une solution.\n",
    "\n",
    "#### Fonction `find_seed`\n",
    "\n",
    "Cette fonction renvoie un flottant éloigné d'au plus `eps`d'un t en utilisant une méthode de dichotomie et `None`si la condition n'est pas vérifiée sur l'intervalle.\n",
    "\n",
    "La fonction se base sur un algorithme de dichotomie.\n",
    "\n",
    "- On part de l'intervalle $[\\min(h(0),h(1)), \\max(h(0),h(1))]$.\n",
    "- Si $c$ n'appartient pas à cet intervalle, on renvoie `None`. Sinon, tant que la longueur de l'intervalle est supérieure à `eps`, on coupe l'intervalle en deux et on prend comme nouvel intervalle celui qui contient $c$. Plus précisément, si $(h(debut)-c)(h(t)-c) \\leqslant 0$ où $t = \\frac{debut - fin}{2}$\n",
    "- On a alors $fin = t$ et $debut = t$ sinon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x,y):\n",
    "    return x**2+y**2+1\n",
    "\n",
    "a=0\n",
    "\n",
    "def h(t):\n",
    "    return f(a,t)\n",
    "    \n",
    "def find_seed(h, c = 0.0, eps = 2**(-26)): \n",
    "    \"\"\"Cette fonction renvoie un flottant éloigné d'au plus eps d'un t en utilisant \n",
    "    une méthode de dichotomie et Nonesi la condition n'est pas vérifiée sur l'intervalle.\"\"\"\n",
    "    debut = 0\n",
    "    fin = 1\n",
    "    if (c<min(h(debut),h(fin))) or (c>max(h(debut),h(fin))): \n",
    "        print('bonjour')\n",
    "        return None\n",
    "    ecart = fin-debut\n",
    "    while (ecart > eps):\n",
    "        t = (debut+fin)/2\n",
    "        if (h(debut)-c)*(h(t)-c)<=0:\n",
    "            fin = t\n",
    "        else:\n",
    "            debut = t\n",
    "        ecart = fin-debut\n",
    "    return t \n",
    "\n",
    "find_seed(h, c = 1.5, eps = 2**(-26))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Propagation\n",
    "\n",
    "*Méthode :* \n",
    "\n",
    "On se place sur l'espace $[0,1]^2$. On a trouvé par dichotomie la **racine de la fonction** sur l'arête gauche gràce à la fonction `find_seed`. On calcule le **gradient** de la fonction, supposé non nul, en ce point puis son **orthogonale**, qui correspond à la **tangente** en ce point de la fonction. On se déplace d'un entier `delta` sur la tangente puis à l'aide de la méthode de Newton, on cherche le point le plus proche qui appartient à la courbe de niveau. \n",
    "\n",
    "*Mise en application :*\n",
    "\n",
    "On cheche à implémenter une fonction qui renvoie un fragment de ligne de niveau de valeur $c$ de $f$ sous la forme de deux tableaux à une dimension d'abscisses et d'ordonnées de points de cette ligne, approximativement espacés de `delta`. Si un tel fragment ne peut être généré, la fonction renvoie deux tableaux vides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit quelques fonctions qui serviront à la lisibilité du code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad(f, x, y):\n",
    "    \"\"\"Renvoie le gradient d'un point de coordonnées (x,y) sous forme de np.array\"\"\"\n",
    "    g = autograd.grad \n",
    "    return np.array([g(f,0)(x,y), g(f,1)(x,y)])\n",
    "    \n",
    "def perpendiculaire(v):\n",
    "    \"\"\"Renvoie le vecteur orthogonal à v \"\"\"\n",
    "    return np.array([v[1],-v[0]])\n",
    "\n",
    "def fin_ligne(x,y,delta):\n",
    "    \"\"\"Prend en argument x et y des array et renvoie un booléen qui indique si le dernier point\n",
    "    de x et y appartient toujours au carré [0,1]x[0,1] à delta près.\"\"\"\n",
    "    if (x[-1] > 1-delta) or (x[-1] < delta) or (y[-1] > 1-delta) or (y[-1] < delta):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def distance(x,y,a,b):\n",
    "    \"\"\"Renvoie la distance euclidienne\"\"\"\n",
    "    return (sqrt((x-a)**2+(y-b)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithme de `simple_contour_propagation` \n",
    "\n",
    "Dans ce cas, les coordonnées trouvées pour le point suivant ne sont pas les bonnes. L'erreur se propage en raison de l'imprécision de la méthode du gradient. \n",
    "\n",
    "![title](newton.jpg)\n",
    "\n",
    "De plus, pour chaque gradient calculé, il existe deux vecteurs orthogonaux possibles. On veillera alors à prendre celui dont la distance par rapport au point précédent était maximale. Il est ensuite nécessaire de le normaliser par la valeur `delta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_contour_propagation(f, c = 0.0, delta=0.01): \n",
    "    \"\"\"Renvoie la liste des points de la courbe de niveau sans faire appel à la méthode de Newton-\"\"\"\n",
    "    x1 = []\n",
    "    y1 = []\n",
    "    a = 0\n",
    "    def h(t):\n",
    "        return f(a,t)\n",
    "    t = find_seed(h,c=1.5,eps=2**(-26))\n",
    "    x1.append(0.0)\n",
    "    y1.append(t) #on a ajouté le premier point du contour sur l'axe x=0\n",
    "    gradient=grad(f,0.0,t)\n",
    "    perp=perpendiculaire(gradient)*delta/np.linalg.norm(gradient)\n",
    "    x1.append(perp[0]+x1[-1])\n",
    "    y1.append(perp[1]+y1[-1])\n",
    "    #là il faudrait faire le newton\n",
    "    while fin_ligne(x1,y1,delta)==True:\n",
    "        gradient=grad(f,float(x1[-1]),float(y1[-1]))\n",
    "        perp=perpendiculaire(gradient)*delta/np.linalg.norm(gradient)\n",
    "        perp2=-perp\n",
    "        #on choisit le \"bon\" contour\n",
    "        if distance(x1[-1],y1[-1],perp[0]+x1[-1],perp[0]+y1[-1]) < distance(x1[-1],y1[-1],perp2[0]+x1[-1],perp2[0]+y1[-1]):\n",
    "            x1.append(perp2[0]+x1[-1])\n",
    "            y1.append(perp2[1]+y1[-1])\n",
    "        else:\n",
    "            x1.append(perp[0]+x1[-1])\n",
    "            y1.append(perp[1]+y1[-1])\n",
    "        #il faudraitfaire le newton\n",
    "    x=np.array(x1)\n",
    "    y=np.array(y1)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **méthode de Newton** permet de trouver le point le plus proche appartenant véritablement à la courbe.\n",
    "\n",
    "*Principe*:\n",
    "De manière générale, si $F$ est une fonction de la forme :\n",
    "\n",
    "$$\\begin{array}{ccccc}\n",
    "F & : & \\mathbb{R}^2 & \\to & \\mathbb{R}^2 \\\\\n",
    "& & (x_1,x_2) & \\mapsto & (f(x_1,x_2),g(x_1,x_2)) \\\\\n",
    "\\end{array}$$\n",
    "\n",
    "On cherche $(S_1,S_2)$ tels que $F(S_1,S_2) = 0_{\\mathbb{R}^2}$\n",
    "\n",
    "On définit la matrice Jacobienne de $F$ par :\n",
    "$\\forall (x_1,x_2) \\in  \\mathbb{R}^2, J(x_1,x_2) =\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial F_1(x_1,x_2)}{\\partial x_1} & \\frac{\\partial F_1(x_1,x_2)}{\\partial x_2} \\\\\n",
    "\\frac{\\partial F_2(x_1,x_2)}{\\partial x_1} & \\frac{\\partial F_2(x_1,x_2)}{\\partial x_2}\n",
    "\\end{pmatrix}$\n",
    "\n",
    "La méthode de Newton consiste donc à calculer les itérations successives de la suite définie par :\n",
    "\n",
    "$$\\begin{pmatrix}\n",
    "x_{1,n+1} \\\\\n",
    "x_{2,n+1}\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "x_{1,n} \\\\\n",
    "x_{2,n}\n",
    "\\end{pmatrix}\n",
    "- J(x_{1,n},x_{2,n})^{-1} \n",
    "\\begin{pmatrix}\n",
    "F_1(x_{1,n},x_{2,n}) \\\\\n",
    "F_2({1,n},x_{2,n})\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Et on montre que \n",
    "$\\lim\\limits_{x \\to \\inf} \n",
    "\\begin{pmatrix}\n",
    "x_{1,n} \\\\\n",
    "x_{2,n}\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "S_1 \\\\\n",
    "S_2\n",
    "\\end{pmatrix}$\n",
    "\n",
    "Dans notre étude, $F$ est définie par :\n",
    "\n",
    "$$\\begin{array}{ccccc}\n",
    "F & : & \\mathbb{R}^2 & \\to & \\mathbb{R}^2 \\\\\n",
    "& & \\begin{pmatrix}\n",
    "x \\\\\n",
    "y\n",
    "\\end{pmatrix} & \\mapsto & \\begin{pmatrix}\n",
    "f(x,y) -c \\\\\n",
    "\\sqrt{(x-x_0)^2,(y-y_0)^2}\n",
    "\\end{pmatrix}\n",
    "\\\\\n",
    "\\end{array}$$\n",
    "\n",
    "où $(x_0,y_0)$ correspond au dernier point de la ligne de niveau stocké en mémoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def newton(f,x,y,c,delta,eps=2**(-26)): \n",
    "    \"\"\"Applique l'algorithme de Newton pour trouver un point (a,b) \n",
    "    situé à une distance de plus de delta du précédent. \n",
    "    L'écart de la fonction f appliquée en (a,b) à c doit être inférieur à eps.\"\"\"\n",
    "    def F(a,b):\n",
    "        return (f(a,b)-c,distance(x,y,a,b))\n",
    "    def J_F(a,b):\n",
    "        j=autograd.jacobian\n",
    "        return np.c_[j(f,0)(a,b),j(f,1)(x,y)]\n",
    "    a=x\n",
    "    b=y\n",
    "    while F(a,b)[0]>eps and F(a,b)[1]>delta:\n",
    "        inverse_jacobienne=np.linlg(J_F(a,b))\n",
    "        inter=(np.array([a,b])-np.dot(inverse_jacobienne,F(a,b)))[0]\n",
    "        b=(np.array([a,b])-np.dot(inverse_jacobienne,F(a,b)))[1]\n",
    "        a=inter\n",
    "    return np.array([a,b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithme de `simple_contour_bug`\n",
    "\n",
    "Ainsi en implémentant la méthode de Newton à l'algorithme précédent, mais sans choisir systématiquement le bon vecteur orthogonal au gradient, on obtient un bug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_contour_bug(f, c = 0.0, delta=0.01):#erreur du au choix de perp\n",
    "    x1 = []\n",
    "    y1 = []\n",
    "    a = 0\n",
    "    def h(t):\n",
    "        return f(a,t)\n",
    "    t = find_seed(h,c=0.3,eps=2**(-26))\n",
    "    x1.append(0.0)#à modifier pour la fct contour\n",
    "    y1.append(t)#on a ajouté le premier point du contour sur l'axe x=0\n",
    "    gradient=grad(f,0.0,t)\n",
    "    perp=perpendiculaire(gradient)*delta/np.linalg.norm(gradient)\n",
    "    x1_inter=perp[0]+x1[-1]\n",
    "    y1_inter=perp[1]+y1[-1]\n",
    "    l=newton(f,x1_inter,y1_inter,2**(-26),delta,c)\n",
    "    x1.append(l[0])\n",
    "    y1.append(l[1])\n",
    "    while fin_ligne(x1,y1,delta)==True:\n",
    "        gradient=grad(f,float(x1[-1]),float(y1[-1]))\n",
    "        perp=perpendiculaire(gradient)*delta/np.linalg.norm(gradient)\n",
    "        perp2=-perp\n",
    "        #on choisit le \"bon\" contour\n",
    "        if distance(x1[-1],y1[-1],perp[0]+x1[-1],perp[0]+y1[-1]) < distance(x1[-1],y1[-1],perp2[0]+x1[-1],perp2[0]+y1[-1]):\n",
    "            x1_inter=perp[0]+x1[-1]\n",
    "            y1_inter=perp[1]+y1[-1]\n",
    "            l=newton(f,x1_inter,y1_inter,2**(-26),delta,c)\n",
    "            x1.append(l[0])\n",
    "            y1.append(l[1])\n",
    "        else:\n",
    "            x1_inter=perp2[0]+x1[-1]\n",
    "            y1_inter=perp2[1]+y1[-1]\n",
    "            l=newton(f,x1_inter,y1_inter,2**(-26),delta,c)\n",
    "            x1.append(l[0])\n",
    "            y1.append(l[1])\n",
    "    x=np.array(x1)\n",
    "    y=np.array(y1)\n",
    "    return x,y\n",
    "\n",
    "\n",
    "f = lambda x,y : x**2 + y**2\n",
    "x,y = simple_contour_bug(f, c=0.3, delta = 0.01)\n",
    "plt.plot(x,y)\n",
    "plt.title(\"Courbe de niveau de niveau de la fonction $x^2 + y^2$ pour $c=0.3$\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithme de `simple_contour`\n",
    "\n",
    "Cette fois, on corrige l'erreur de l'algorithme précédent en choisissant le bon vecteur perpendiculaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_contour(f, c = 0.0, delta=0.01):\n",
    "    x1 = []\n",
    "    y1 = []\n",
    "    test=True #ce booléen permettra de choisir quel vecteur perpendiculaire choisir\n",
    "    a = 0\n",
    "    def h(t):\n",
    "        return f(a,t)\n",
    "    t = find_seed(h,c=0.3,eps=2**(-26))\n",
    "    x1.append(0.0)\n",
    "    y1.append(t)#on a ajouté le premier point du contour sur l'axe x=0\n",
    "    gradient=grad(f,0.0,t)\n",
    "    perp=perpendiculaire(gradient)*delta/np.linalg.norm(gradient)\n",
    "    x1_inter=perp[0]+x1[-1]\n",
    "    y1_inter=perp[1]+y1[-1]\n",
    "    l=newton(f,x1_inter,y1_inter,2**(-26),delta,c)\n",
    "    x1.append(l[0])\n",
    "    y1.append(l[1])\n",
    "    test=fin_ligne(x1,y1,delta)\n",
    "    if test==True:\n",
    "        while fin_ligne(x1,y1,delta)==True: #même commentaire que pour x1.append(0.0)\n",
    "            gradient=grad(f,float(x1[-1]),float(y1[-1]))\n",
    "            perp=perpendiculaire(gradient)*delta/np.linalg.norm(gradient)\n",
    "            x1_inter=perp[0]+x1[-1]\n",
    "            y1_inter=perp[1]+y1[-1]\n",
    "            l=newton(f,x1_inter,y1_inter,2**(-26),delta,c)\n",
    "            x1.append(l[0])\n",
    "            y1.append(l[1])\n",
    "    else:#on choisit l'autre vecteur perpendiculaire\n",
    "        x1.pop()\n",
    "        y1.pop()\n",
    "        x1_inter=perp[0]+x1[-1]\n",
    "        y1_inter=perp[1]+y1[-1]\n",
    "        l=newton(f,x1_inter,y1_inter,2**(-26),delta,c)\n",
    "        x1.append(l[0])\n",
    "        y1.append(l[1])\n",
    "        while fin_ligne(x1,y1,delta)==True:\n",
    "            gradient=grad(f,float(x1[-1]),float(y1[-1]))\n",
    "            perp=perpendiculaire(gradient)*delta/np.linalg.norm(gradient)\n",
    "            perp2=-perp\n",
    "            x1_inter=perp2[0]+x1[-1]\n",
    "            y1_inter=perp2[1]+y1[-1]\n",
    "            l=newton(f,x1_inter,y1_inter,2**(-26),delta,c)\n",
    "            x1.append(l[0])\n",
    "            y1.append(l[1])\n",
    "    x=np.array(x1)\n",
    "    y=np.array(y1)\n",
    "    return x,y\n",
    "\n",
    "f = lambda x,y : x**2+y**2\n",
    "\n",
    "x,y = simple_contour(f, c=0.3, delta = 0.01)\n",
    "plt.plot(x,y)\n",
    "plt.title(\"Courbe de niveau de niveau de la fonction $x^2 + y^2$ pour $c=0.3$\")\n",
    "plt.axis('equal')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
